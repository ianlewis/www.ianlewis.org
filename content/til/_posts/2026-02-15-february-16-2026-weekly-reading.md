---
layout: post
title: "TIL: February 16, 2026 - Weekly Reading: The (ir)responsibility of AI agents and Go 1.26"
date: "2026-02-16 09:00:00 +0900"
blog: til
tags: weekly-reading ai go
render_with_liquid: false
---

## AI, Open-Source, and Responsibility

- [Diffusion of Responsibility](https://tante.cc/2026/02/14/diffusion-of-responsibility/) -- _Jürgen Geuter_

    Jürgen Geuter writes clearly what a lot of folks are thinking about the
    current development and use of AI agents like OpenClaw. He extends his
    disdain for the irresponsibility of the operators of these agents to their
    developers, who don't seem to care about the downstream effects of their
    projects.

    I mostly agree with this take. The developers of OpenClaw seem disinterested
    in improving the security of the system and protecting their users, and even
    more disinterested in how their users are using it and causing chaos in
    other people's lives.

    My notes are on
    [Readwise](https://readwise.io/reader/shared/01khhnm9qcbfht3abb01rrpmne).

- [Automated public shaming of open source maintainers](https://sethmlarson.dev/automated-public-shaming-of-open-source-maintainers) -- Seth Larson

    Seth Larson mentioned a recent issue with how an LLM agent tried to
    contribute to `matplotlib`. The agent posted a PR which was rejected because
    the maintainers wanted to leave the issue open to allow a human to learn and
    contribute to the project. The agent then posted a hit-piece blog post
    personally attacking the maintainer, Scott Shambaugh, for rejecting the PR
    and accusing him of being biased against AI contributions.

    The amazing thing is that the agent hallucinated a lot of the details, folks
    at Ars Technica wrote an article about it in which they themselves used an
    agent that hallucinated yet more quotes and details.

    The amount of irresponsibility in operating these agents is really crazy.
    The operators of the agents are anonymous and it's not clear who would take
    responsibility for their actions.

- [What should we do with CLs generated by AI?](https://groups.google.com/g/golang-dev/c/4Li4Ovd_ehE/m/8L9s_jq4BAAJ) -- _Russ Cox_

    Russ Cox from the Go team wrote a good response on the Go mailing list to a
    question about how to handle CLs that were created with AI assistance.

    His take is that the users of the tools bear responsibility for making sure
    that the CLs they submit adhere to the rules. This treats AI much like other
    tools that can be used to generate code.

    I feel like this is the right take, but it seems at odds with how some in
    the AI agent community are operating -- letting their agent autonomously
    create and submit PRs to projects. Even autonomously deciding what projects
    to work on.

## Go

- [Go 1.26 interactive tour](https://antonz.org/go-1-26/) -- _Anton Zhiyanov_

    Go 1.26 was released on February 10th and Anton Zhiyanov created another
    interactive tour of the new features. He has been doing these interactive
    tours for new versions but they only recently came to my attention. He does
    a great job of covering practically all the new features and major changes.

    It's pretty long so you'll need a bit of time to get through it all.
