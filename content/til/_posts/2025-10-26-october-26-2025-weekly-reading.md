---
layout: post
title: "TIL: October 26, 2025 - Weekly Reading: Sora 2 and the AI Bubble"
date: "2025-10-26 00:00:00 +0900"
blog: til
tags: weekly-reading ai
render_with_liquid: false
---

## Sora 2 and the AI Bubble(?)

This week I watched several videos discussing Sora 2, OpenAI's TikTok clone where
users create short AI slop videos using real people's likenesses. The hubris and
obvious ethical issues really make me think that there is a kind of "ethics
doesn't matter, we'll just do whatever", grifter nihilism at play with tech
business leaders.

- [<!-- textlint-disable spelling -->SORA: the all Ai TikTok Clone. will slop end creativity?<!-- textlint-enable spelling -->](https://www.youtube.com/watch?v=I1dW-nZqhew) -- _Casey Neistat_

    Casey covers the content creator perspective, and how Sora 2 will likely
    flood the market with low-effort low-quality AI-generated content. Casey's
    key message is that friction in the creation process limited the volume of
    bad content you would see. More content going into the funnel doesn't
    necessarily mean more good content coming out.

    > _It makes self-expression available to everyone, and I love that. But what
    > happens when you remove the entire process? What happens when all you have
    > to do is like type a couple of words from your bed in a dark room and
    > click a button, and it gives you a piece of video? Then you share it, and
    > then you do that a thousand times a day._

    As good as Casey is, I think he _could_ explain it a bit better. The way I
    see it, the friction in the creation process forces creators to be
    _creative_ (who would have thought?) and put effort into the quality of the
    work. That friction is the heat which forges good content. It's less about
    the volume and more about the required effort.

    **Aside:** Casey is just so good at making engaging videos. From the cream
    of mushroom soup can representing AI slop, to recording himself buying the
    funnel that he uses in an explanation later in the video, to his use of
    AI-slop to provide examples and express ideas. ðŸ¤Œ

- [Sora Proves the AI Bubble Is Going to Burst So Hard](https://www.youtube.com/watch?v=55Z4cg5Fyu4) -- _Adam Conover_

    Adam goes into the ethical issues with Sora 2 as well as the business
    aspect. Adam's videos are commentary, but he does a decent job of adding
    sources.

    He makes some of the economic argument well. OpenAI and others are
    going to have to make so much money to justify the enormous investments
    they're making in data centers and infrastructure, that it's hard to see how
    they do that with their current products and business models.

    > _... in order to pay for all the processing infrastructure they need to
    > build this brave screwed world,
    > [Bain estimates](https://www.bain.com/about/media-center/press-releases/20252/$2-trillion-in-new-revenue-needed-to-fund-ais-scaling-trend---bain--companys-6th-annual-global-technology-report/)
    > AI companies will have to earn $2 trillion annually by 2030. According to
    > the Wall Street Journal, That is more than the combined 2024 revenue of
    > Amazon, Apple, Alphabet, Microsoft, Meta, and NVIDIA combined._

    It's hard to square the existence of a product like Sora 2 that has a high
    cost to run, with the high-value things OpenAI has promised it could be
    doing instead. Like curing diseases, solving climate change, or automating
    jobs. It makes you think that maybe they aren't really up to the task so
    they are grasping at low-hanging fruit (or rotten fruit that's falling off
    the tree).

- [<!-- textlint-disable spelling -->Give me a single reason why Sora2 should exist.<!-- textlint-enable spelling -->](https://www.youtube.com/watch?v=Vz0oQ0v0W10) -- _Hank Green_

    Hank's video isn't the best produced. It's basically him ranting, but he
    makes some good points. He does make similar ethical points. All three
    videos touch on the use of real people, the lack of consent, and the sheer
    ease of creating content that is misleading or slanderous.

    He points out an interesting thought that previously the argument was that
    services weren't responsible for user-generated content. But with AI the
    platform actually has a hand in creating the content so do they also bear
    some responsibility for it?

    > _And if you trot out section 230 of the Communications Decency Act and say
    > we didn't do that, the person put in the prompt. It's not OpenAI's fault.
    > We can't be held responsible for the content of our platform even though
    > our computers enabled its creation and also did the actual creation of
    > turning a single sentence into that monstrosity._

- [The State of the AI Industry is Freaking Me Out](https://www.youtube.com/watch?v=Q0TpWitfxPk) -- _Hank Green_

    Hank made a second video that is more general about the AI industry and how
    it's funding itself. It's not terribly well researched but it does note a
    pattern in how companies are investing and lending to each other in a kind
    of closed ecosystem that seems unsustainable. The whole thing seems propped
    up by HUGE amounts of investment money which is chasing a return that has
    a worryingly high chance of never materializing.

    He points out is hard to believe that OpenAI selling its end product, making
    $12 billion a year, is supposed to sustain hundreds of billions of dollars
    of investment in infrastructure going to NVIDIA and data center
    construction.

    > _... the chip isn't the product. The AI service to consumers is the
    > product. And right now, that is an order of magnitude under the amount of
    > money that is being spent just on hardware. Not even including staff time
    > and the power and the water and the buildings and all of that stuff. The
    > amount of money being made by AI companies is an order of magnitude lower
    > than just the money that is going to NVIDIA every year._

    Hank responded to some comments on this video in a [follow-up
    video](https://www.youtube.com/watch?v=mNr06IqJdnQ) where he clarifies a few
    points. Notably, he points out that he thinks AI is a transformative
    technology. It just won't generate the kind of returns that justify current
    investment levels. He also seems to think this strongly enough that he's
    [changed his investment
    strategy](https://www.youtube.com/watch?v=VZMFp-mEWoM) to diversify away
    from the S&P 500 because of its heavy exposure to AI-related stocks.

- [Bubble, Bubble, Toil and Trouble](https://thezvi.substack.com/p/bubble-bubble-toil-and-trouble) -- _Zvi Mowshowitz_

    This article provides a bit of counterbalance to the AI bubble doom and
    gloom videos above. Zvi finds the bubble arguments weak and unconvincing.
    This is partly due to the fact that the Nasdaq forward P/E ratio is
    something like 28x compared to 70x during the dot-com bubble and the track
    record of growth with AI companies is strong so far.

    I think the danger is that the economy is a bit weak aside from the AI
    investments, and the expectations are just so high that it's hard to believe
    they will sustain the expected growth given the relative costs.
